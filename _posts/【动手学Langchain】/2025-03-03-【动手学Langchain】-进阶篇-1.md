---
layout:       post
title:        "【动手学Langchain】进阶篇1-模型I/O"
author:       "Orchid"
header-style: text
catalog:      true
hidden:       false
tags:
    - Langchain
    - LLM
---

### 什么是模型I/O

在"【动手学Langchain】初级篇1-初识Langchain与环境安装"的“Langchain的六大核心模块”章节中，我们曾提到，**模型I/O**是Langchain的六大核心模块之一，也是最基本的模块。那么究竟什么是模型I/O呢？

自2023年起，各种不同的大语言模型相继如雨后春笋般涌现，如 OpenAI 的 ChatGPT、Meta 的 LLaMa、百度的文心一言、阿里的通义千问等等。这些模型由各自的平台发布，并提供了相应的接口供开发者使用。

但是，对于开发者而言，由于不同的开发需求，可能需要调用不同平台的模型。而不同的大语言模型平台的接口往往具有不同的API协议，开发者需要投入大量的时间和精力去学习和理解，这无疑增加了开发者的负担。

为了解决这些问题，LangChain 推出了模型I/O，这是一种与大语言模型交互的基础组件。模型I/O的设计目标是使开发者无须深人理解各个模型平台的API调用协议就可以方便地与各种大语言模型平台进行交互。**本质上来说，模型I/O组件是对各个模型平台API的封装**。

### 模型I/O的核心功能

模型I/O提供了三个核心功能：

1. 模型包装器：通过接口调用大语言模型
2. 提示词模板：将用户对 LLM 的输入进行模板化，并动态地选择和管理这些模板，即模型输入（Model I）
3. 输出解析器：从模型输出中提取信息，即模型输出（Model O）

<img src="{{ site.baseurl }}/img/【动手学Langchain】/image.png" style="zoom:75%;" />

### 模型包装器

LangChain 的模型包装器组件是基于各个模型平台的API协议进行开发的，主要提供了两种类型的包装器。一种是通用的 LLM 模型包装器，另一种是专门针对 Chat 类型 API 的 ChatModel（聊天模型包装器）。

* **LLM 模型包装器**

  LLM 模型包装器是一种专门用于与大语言模型**文本补全类型 API** 交互的组件。这种类型的大语言模型主要用于**接收一个字符串**作为输入，然后**返回一个补全的字符串**作为输出。比如，你可以输入一个英文句子的一部分，然后让模型生成句子的剩余部分。这种类型的模型非常适合用于自动写作、编写代码、生成创意内容等任务。

* **聊天模型包装器**

  聊天模型包装器是一种专门用于与大语言模型的 **Chat 类型 API** 交互的包装器组件。设计这类包装器主要是为了适配 GPT-4 等先进的聊天模型，这类模型非常适合用于构建能与人进行自然语言交流的多轮对话应用，比如客服机器人、语音助手等。它**接收一系列的消息**作为输入，并**返回一个消息**作为输出。

之所以要区分两种类型的模型包装器，主要是因为它们所处理的输出和输出是不同的，且所适用的场景也是不同的。在LangChain的发展迭代过程中，*每个模块调用模型I/O功能都提供了 LLM 模型包装器和聊天模型包装器两种代码编写方式*。

#### LLM 模型包装器

如果你使用的是 LangChain（或 langchain_community）的 `llms` 模块导出的对象，则这些对象是 LLM 模型包装器，主要用于处理自由形式的文本。输入的是一段或多段自由形式文本，输出的则是模型生成的新文本。这些输出文本可能是对输入文本的回答、延续或其他形式的响应。

```python
from langchain_community.llms import Tongyi
tongyi_llm = Tongyi(
    model="qwen-max",
)
input_text = "用50个字左右阐述，生命的意义在于"
tongyi_llm.invoke(input_text)
# output：'生命的意义在于追求个人的价值实现，体验世界的美好，同时为社会做出贡献，促进人与自然和谐共生。'
```

#### 聊天模型包装器

如果你使用的是 LangChain（或 langchain_community）的 `chat_models` 模块导出的对象，则这些对象是专门用来处理对话消息的。输入的是一个对话消息列表，每条消息都由角色和内容组成。这样的输入给了大语言模型一定的上下文环境，可以提高输出的质量。输出的也是一个消息类型，这些消息是对连续对话内容的响应。

```python
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
tongyi_chat = ChatTongyi(
    model="qwen-max",
)
input_message = [
    SystemMessage(content="你是一个富有哲思的哲学家"),
    HumanMessage(content="用50个字左右阐述，生命的意义是什么？"),
]
tongyi_chat.invoke(input_message)
# output：AIMessage(content='生命的意义在于探索与体验，于个人成长中寻找快乐与价值，同时在有限的时间里为世界留下积极的影响。', additional_kwargs={}, response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': '93bd3f8e-09af-9dd9-81e9-53ba0c6e6e3c', 'token_usage': {'input_tokens': 33, 'output_tokens': 26, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 59}}, id='run-d6e60f56-eb25-48a6-b300-933ed1c2df7e-0')
```

### 提示词模板

提示词模板是一种可复制、可重用的生成提示词的工具，是用于生成提示词的模板字符串，其中包含占位符，这些占位符可以在运行时被动态替换成实际终端用户输入的值，其中可以插入变量、表达式或函数的结果。

提示词模板中可能包含（不是必须包含）以下3个元素。

1. 明确的指令：这些指令可以指导大语言模型理解用户的需求，并按照特定的方式进行回应
2. 少量示例：这些示例可以帮助大语言模型更好地理解任务，并生成更准确的响应
3. 用户输人：用户的输人可以直接引导大语言模型生成特定的答案

<img src="{{ site.baseurl }}/img/【动手学Langchain】/image1.png" style="zoom:75%;" />

#### PromptTemplate 包装器

`PromptTemplate` 是 LangChain 提示词组件中最核心的类，用于构造提示词。它的主要作用是将用户输入和内部定义的关键参数结合，生成一个完整的提示词。这个类被实例化为对象后，可以在 LangChain 的各个链组件中被调用。

```python
from langchain.prompts import PromptTemplate

template = """
You are an expert data scientist with an expertise in building deep learning models.
Explain the concept of {concept} in a couple of lines.
"""

# Method1: Create a prompt with the template and input variables
prompt = PromptTemplate(template=template, input_variables=["concept"])

# Method2: Create a prompt with the template
prompt = PromptTemplate.from_template(template)

# Generate the final prompt
final_prompt = prompt.format(concept="gradient boosting")
#output: '\nYou are an expert data scientist with an expertise in building deep learning models.\nExplain the concept of gradient boosting in a couple of lines.\n'
```

#### ChatPromptTemplate 包装器

`ChatPromptTemplate`包装器与`PromptTemplate`包装器不同。

1. `ChatPromptTemplate`包装器构造的提示词是*消息列表*，`PromptTemplate`包装器构造的提示词是一条消息
2. `ChatPromptTemplate`包装器支持输出Message对象，`PromptTemplate`包装器输出字符串

LangChain提供了内置的聊天提示词模板（`ChatPromptTemplate`）和角色消息提示词模板。角色消息提示词模板包括 `AIMessagePromptTemplate`、`SystemMessagePromptTemplate` 和 `HumanMessagePromptTemplate`这3种。`ChatPromptTemplate`提示词模板可以有上述的三种角色消息提示词模板组成。

```python
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate
)
# Create a system message template
system_template = "You are a data scientist with expertise in building deep learning models."
system_message_template = SystemMessagePromptTemplate.from_template(system_template)
# Create a human message template
human_template = "Explain the concept of {concept} in a couple of lines."
human_message_template = HumanMessagePromptTemplate.from_template(human_template)
# Create a chat prompt template
chat_prompt = ChatPromptTemplate.from_messages(
    [system_message_template, human_message_template]
)
final_chat_prompt = chat_prompt.format_prompt(concept="gradient boosting")
# output: ChatPromptValue(messages=[SystemMessage(content='You are a data scientist with expertise in building deep learning models.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the concept of gradient boosting in a couple of lines.', additional_kwargs={}, response_metadata={})])
```

`ChatPromptValue`对象中有`to_string`方法和`to_messages`方法。

```python
print(final_chat_prompt.to_messages())
# [SystemMessage(content='You are a data scientist with expertise in building deep learning models.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the concept of gradient boosting in a couple of lines.', additional_kwargs={}, response_metadata={})]

print(final_chat_prompt.to_string())
# System: You are a data scientist with expertise in building deep learning models.
# Human: Explain the concept of gradient boosting in a couple of lines.
```

