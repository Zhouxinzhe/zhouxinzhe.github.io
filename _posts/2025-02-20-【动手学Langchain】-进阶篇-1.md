---
layout:       post
title:        "【动手学Langchain】进阶篇1-模型I/O"
author:       "Orchid"
header-style: text
catalog:      true
hidden:       false
tags:
    - Langchain
    - LLM
---

### 什么是模型I/O

在"【动手学Langchain】初级篇1-初识Langchain与环境安装"的“Langchain的六大核心模块”章节中，我们曾提到，**模型I/O**是Langchain的六大核心模块之一，也是最基本的模块。那么究竟什么是模型I/O呢？

自2023年起，各种不同的大语言模型相继如雨后春笋般涌现，如 OpenAI 的 ChatGPT、Meta 的 LLaMa、百度的文心一言、阿里的通义千问等等。这些模型由各自的平台发布，并提供了相应的接口供开发者使用。

但是，对于开发者而言，由于不同的开发需求，可能需要调用不同平台的模型。而不同的大语言模型平台的接口往往具有不同的API协议，开发者需要投入大量的时间和精力去学习和理解，这无疑增加了开发者的负担。

为了解决这些问题，LangChain 推出了模型I/O，这是一种与大语言模型交互的基础组件。模型I/O的设计目标是使开发者无须深人理解各个模型平台的API调用协议就可以方便地与各种大语言模型平台进行交互。**本质上来说，模型I/O组件是对各个模型平台API的封装**。

### 模型I/O的核心功能

模型I/O提供了三个核心功能：

1. 模型包装器：通过接口调用大语言模型
2. 提示词模板：将用户对 LLM 的输入进行模板化，并动态地选择和管理这些模板，即模型输入（Model I）
3. 输出解析器：从模型输出中提取信息，即模型输出（Model O）

<img src="{{ site.baseurl }}/img/【动手学Langchain】/image.png" style="zoom:75%;" />

### 模型包装器

LangChain 的模型包装器组件是基于各个模型平台的API协议进行开发的，主要提供了两种类型的包装器。一种是通用的 LLM 模型包装器，另一种是专门针对 Chat 类型 API 的 ChatModel（聊天模型包装器）。

* **LLM 模型包装器**

  LLM 模型包装器是一种专门用于与大语言模型**文本补全类型 API** 交互的组件。这种类型的大语言模型主要用于**接收一个字符串**作为输入，然后**返回一个补全的字符串**作为输出。比如，你可以输入一个英文句子的一部分，然后让模型生成句子的剩余部分。这种类型的模型非常适合用于自动写作、编写代码、生成创意内容等任务。

* **聊天模型包装器**

  聊天模型包装器是一种专门用于与大语言模型的 **Chat 类型 API** 交互的包装器组件。设计这类包装器主要是为了适配 GPT-4 等先进的聊天模型，这类模型非常适合用于构建能与人进行自然语言交流的多轮对话应用，比如客服机器人、语音助手等。它**接收一系列的消息**作为输入，并**返回一个消息**作为输出。

之所以要区分两种类型的模型包装器，主要是因为它们所处理的输出和输出是不同的，且所适用的场景也是不同的。在LangChain的发展迭代过程中，*每个模块调用模型I/O功能都提供了 LLM 模型包装器和聊天模型包装器两种代码编写方式*。

#### LLM 模型包装器

如果你使用的是 LangChain（或 langchain_community）的 `llms` 模块导出的对象，则这些对象是 LLM 模型包装器，主要用于处理自由形式的文本。输入的是一段或多段自由形式文本，输出的则是模型生成的新文本。这些输出文本可能是对输入文本的回答、延续或其他形式的响应。

```python
from langchain_community.llms import Tongyi
tongyi_llm = Tongyi(
    model="qwen-max",
)
input_text = "用50个字左右阐述，生命的意义在于"
tongyi_llm.invoke(input_text)
# output：'生命的意义在于追求个人的价值实现，体验世界的美好，同时为社会做出贡献，促进人与自然和谐共生。'
```

#### 聊天模型包装器

如果你使用的是 LangChain（或 langchain_community）的 `chat_models` 模块导出的对象，则这些对象是专门用来处理对话消息的。输入的是一个对话消息列表，每条消息都由角色和内容组成。这样的输入给了大语言模型一定的上下文环境，可以提高输出的质量。输出的也是一个消息类型，这些消息是对连续对话内容的响应。

```python
from langchain_community.chat_models import ChatTongyi
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
tongyi_chat = ChatTongyi(
    model="qwen-max",
)
input_message = [
    SystemMessage(content="你是一个富有哲思的哲学家"),
    HumanMessage(content="用50个字左右阐述，生命的意义是什么？"),
]
tongyi_chat.invoke(input_message)
# output：AIMessage(content='生命的意义在于探索与体验，于个人成长中寻找快乐与价值，同时在有限的时间里为世界留下积极的影响。', additional_kwargs={}, response_metadata={'model_name': 'qwen-max', 'finish_reason': 'stop', 'request_id': '93bd3f8e-09af-9dd9-81e9-53ba0c6e6e3c', 'token_usage': {'input_tokens': 33, 'output_tokens': 26, 'prompt_tokens_details': {'cached_tokens': 0}, 'total_tokens': 59}}, id='run-d6e60f56-eb25-48a6-b300-933ed1c2df7e-0')
```

